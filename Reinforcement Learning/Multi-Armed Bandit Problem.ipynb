{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Armed Bandit Problem\n",
    "\n",
    "Reinforcement learning is a powerful branch of machine learning which uses data observed from time _T_ to determine what action to take at time _T + 1_. This ideology stems from the _Multi-Armed Bandit_ problem, which questions how to achieve the best possible outcome. Many important algorithms such as the _Upper Confidence Bound_ and _Thompson Sampling_ algorithms were created to solve this exact problem.\n",
    "\n",
    "## Exploitation vs. Exploration\n",
    "\n",
    "\n",
    "* __Exploitation__: Going for the best circumstance determined by previous actions.\n",
    "\n",
    "\n",
    "* __Exploration__: Figuring out the best cirumstance possible. Exploitation cannot be achieved without exploration.\n",
    "\n",
    "Exploitation and exploration can be applied to our current lives, such as when we go to restaurants. Exploitation occurs when we go to our favorite restaurant, but miss out on the chance to discover an even better restaurant. Exploration occurs when we go to restaurants we haven't been to before, with the chance of either hating or liking the food there. \n",
    "\n",
    "## The Problem\n",
    "\n",
    "<img src=\"probabilities.png\" style=\"width: 800px;\" alt=\"Different probabilistic outcomes of different slot machines.\" />\n",
    "\n",
    "* There are _K_ slot-machines.\n",
    "\n",
    "\n",
    "* Each slot-machine has its own probabilistic outcome.\n",
    "\n",
    "\n",
    "* Given that _R_ is the numerical representation of total sucesses, pulling on a slot machine will cause one of two outcomes: R++ indicating sucess, or R += 0 indicating failure. \n",
    "\n",
    "\n",
    "* __The Objective:__ Given _N_ number of turns, maximize the value of _R_.\n",
    "\n",
    "## Explanation\n",
    "\n",
    "The reason the multi-armed bandit problem exists is to address the problem of exploiting versus exploring. Exploring helps figure out the slot-machine that produces the best outcome, but it involves searching through K-1 slot machines that don't produce the best outcome. Exploiting involves going for the best slot-machine determined by exploration, but what if the current machine is one of the K-1 slot machines that don't produce the best outcome? Essentially, when is it okay to assume that it is probabilistically safe to stop explore and start exploiting? This is the main basis that inspired many current reinforcement learning algorithms. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
