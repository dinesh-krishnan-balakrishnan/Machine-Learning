{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Importing Libraries\n",
    "\n",
    "A few main libraries used for machine learning are:\n",
    "\n",
    "* __NumPy__ (Numeric Python): Used to perform mathematical and scientific operations, as well as data manipulation.\n",
    "* __Matplotlib__ (Mathematical Plotting Library): Generates various plots to help visualize a dataset.\n",
    "* __Pandas__: The main data science library used for data analysis.\n",
    "\n",
    "__To import the libraries:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Importing the Dataset\n",
    "\n",
    "The pandas library has __read_csv__, which is a function that reads in a CSV file and converts it into a Panda's DataFrame object. The value's in the object can then be split into _x_ and _y_ values. The indexing syntax is: `[rows, columns]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, nan],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', nan, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('Data.csv');\n",
    "\n",
    "X = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, 3].values\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Missing Values\n",
    "\n",
    "The __scikit-learn__ library is automatically contained within NumPy. Its _SimpleImputer_ class helps prevent errors caused by missing data by replacing them with appropriate values. In the example below, the imputer is initialized to take the mean of columns and fill in the missing data. The mean is calculated through the _fit_ method and is then _transformed_ onto the _X_ dataset. By using the mean, the average of the column will still remain the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['France', 44.0, 72000.0],\n",
       "       ['Spain', 27.0, 48000.0],\n",
       "       ['Germany', 30.0, 54000.0],\n",
       "       ['Spain', 38.0, 61000.0],\n",
       "       ['Germany', 40.0, 63777.77777777778],\n",
       "       ['France', 35.0, 58000.0],\n",
       "       ['Spain', 38.77777777777778, 52000.0],\n",
       "       ['France', 48.0, 79000.0],\n",
       "       ['Germany', 50.0, 83000.0],\n",
       "       ['France', 37.0, 67000.0]], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n",
    "imputer = imputer.fit(X[:, 1:3])\n",
    "X[:, 1:3] = imputer.transform(X[:, 1:3])\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Training and Testing\n",
    "\n",
    "A machine learning algorithm needs to be tested to see whether the results of its training has worked. As a result, some data within the data set needs to be split for testing, a common amount being __20%__. To achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0, 0.0, 30.0, 54000.0],\n",
       "       [1.0, 0.0, 50.0, 83000.0]], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __random_state__ operator parameter helps perform a pseudo-random selection of the set, where the data will be split the exact same way if the same __random_state__ value is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Feature Scaling\n",
    "\n",
    "Also known as *standardization* or *normalization*, __Feature Scaling__ helps normalize data within a particular range. This is very important when dealing with algorithms that are negatively affected by large and misleading magnitudes, such as the __Euclidean Distance Algorithm__. __Feature Scaling__ helps converge the data into a much smaller range, which improves the effectiveness of magnitude-dependent algorithms. Additionally, __Feature Scaling__ also helps an algorithm converge towards an output much faster. This is especially true with _matplotlib_.The scaling must be performed after splitting the dataset into a *training* and *test* set, because it calculates the mean and varaince of the entire dataset. This would mean that, if the dataset isn't split, the *training* set values would be affected by the *test* set values and vice versa.\n",
    "<br><br>\n",
    "There are two main forms of feature scaling:\n",
    "\n",
    "### Standardization\n",
    "\n",
    "$$\\large x_{stand} = \\frac{x - mean(x)}{standard\\;deviation(x)}$$\n",
    "\n",
    "### Normalization\n",
    "\n",
    "$$\\large x_{norm} = \\frac{x - min(x)}{max(x) - min(x)}$$\n",
    "\n",
    "Normalization should be used when the entire dataset has the characterisitics of a normal distribution, but standardization pretty much works with all cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.64575131, -0.77459667,  0.26306757,  0.12381479],\n",
       "       [-0.37796447, -0.77459667, -0.25350148,  0.46175632],\n",
       "       [-0.37796447,  1.29099445, -1.97539832, -1.53093341],\n",
       "       [-0.37796447,  1.29099445,  0.05261351, -1.11141978],\n",
       "       [-0.37796447, -0.77459667,  1.64058505,  1.7202972 ],\n",
       "       [-0.37796447,  1.29099445, -0.0813118 , -0.16751412],\n",
       "       [-0.37796447, -0.77459667,  0.95182631,  0.98614835],\n",
       "       [-0.37796447, -0.77459667, -0.59788085, -0.48214934]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scX = StandardScaler()\n",
    "X_train = scX.fit_transform(X_train)\n",
    "X_test = scX.fit_transform(X_test)\n",
    "\n",
    "X_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
