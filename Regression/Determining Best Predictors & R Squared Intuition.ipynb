{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining the Best Predictors\n",
    "\n",
    "When creating a model, there may be variables that don't really have an impact on the results of an algorithm. There may be times where these variables should be included, but there may also be times where they may be unwanted factors in understanding these results. Here are five methods to create the right models based on having the best predictors:\n",
    "\n",
    "__All-In:__ <br> \n",
    "    The all-in method is to just include all the variables in the model. This should only be done if background knowledge shows that the variables present in the dataset are true predictors of the results or are necessary factors to consider.\n",
    "\n",
    "__Backward Elimination:__\n",
    "1. Select a significance level. (EX: $\\alpha$ = 0.05)\n",
    "2. Fit the model with all possible predictors.\n",
    "3. Consider the predictor with the highest P-value. If the P-value is greater than the significance level, it doesn't have a great impact on the dataset. As a result, remove the predictor.\n",
    "4. Continue step 3 until all predictors have a P-value that are less than the significance level.\n",
    "\n",
    "__Forward Selection:__\n",
    "1. Select a significance level.\n",
    "2. Create all regression models with just a single predictor.\n",
    "3. Choose the model with the lowest P-value, and create all models with an extra predictor in addition to the one already present.\n",
    "4. Once more choose the model with newly added predictor that has the lowest P-value.\n",
    "5. Repeat until all predictors are added into the model, or another added predictor would have a P-value higher than the significance level. \n",
    "\n",
    "__Bidirectional Elimination (Stepwise Regression):__ \n",
    "1. Select a significance level for variables to enter and stay in the model. (EX: $\\alpha_{add}$ = 0.05,  $\\alpha_{retain}$ = 0.1)\n",
    "2. Perform a step of forward selection using $\\alpha_add$\n",
    "3. Perform all steps of backward elimination for the current model using $\\alpha_retain$.\n",
    "4. Repeat until all predictors are added into the model, or no more predictors would retain the significance values.\n",
    "\n",
    "\n",
    "__All Possible Models (Score Comparison):__\n",
    "1. Choose a criterion of goodness of fit (EX: Akaike Criterion)\n",
    "2. Construct all possible regression models. Amount equal to $2^n -1$ models.\n",
    "3. Select the model that best fulfills the criterion.\n",
    "\n",
    "<hr>\n",
    "\n",
    "# R$^2$ & Adjusted R$^2$ Intuition\n",
    "\n",
    "__R$^2$:__\n",
    "\n",
    "As previously studied, when a program performs performs best-fit, it creates multiple regressors and determines the best regressor based on the following formula:\n",
    "\n",
    "<span style=\"display: flex;\"> <span style=\"float: left;\"> $$\\LARGE SS_{res} = {\\sum_{i=0}^n (y_i - \\hat{y}_i)^2}$$ </span> <img src=\"linearRegression2.png\" alt=\"Average Regressor\" style=\"width: 500px; float: right;\"/> </span>\n",
    "\n",
    "$SS_{res}$ stands for Residual Sum of Squares, which calculates the error coefficient of a best-fitting regressor. In addition to the Residual Sum of Squares, there also exists the Total Sum of Squares.\n",
    "\n",
    "<span style=\"display: flex;\"> <span style=\"float: left;\"> $$\\LARGE SS_{tot} = {\\sum_{i=0}^n (y_i - {y}_{avg})^2}$$ </span> <img src=\"averageRegressor.png\" alt=\"Average Regressor\" style=\"width: 500px; float: right;\"/> </span>\n",
    "\n",
    "The Total Sum of Squares is simply the general variance of the dataset, while the Residual Sum of Squares is the variance of the dataset in respect to the best-fitting regressor. The $R^2$ equation is based off of the relationship between these two.\n",
    "\n",
    "$$\\LARGE R^2 = {1 - \\frac {SS_{res}}  {{SS}_{tot}}}$$\n",
    "\n",
    "Essentially, $R^2$ is a statistical measure that represents the proportion of variance for a dependent variable. The smaller the Residual Sum of Squares is, the higher the $R^2$ value is. In fact, the most optimal value for a $R^2$ test is 1. If the value is negative, it's a guarantee that the model used is broken.\n",
    "\n",
    "<hr>\n",
    "\n",
    "__Adjusted R$^2$:__ \n",
    "\n",
    "While $R^2$ does a great job of estimating how powerful a prediction is, it does have a flaw when dealing with polynomial regression. With each predictor added:\n",
    "\n",
    "$$\\LARGE SS_{res} \\rightarrow 0 \\text{ & } R^2 \\rightarrow 1$$ \n",
    "\n",
    "It doesn't matter what the predictor is and whether it is related to the dependent variable. All that needs to be done is adjust the coefficient of the new predictor to achieve better results. In fact, if the new variable hinders the dataset, it's coefficient can be set to 0. Because of this, here is a new equation that reduces the $R^2$ value for unwanted predictors. \n",
    "\n",
    "$$\\LARGE \\text{Adj } R^2 = {1 - (\\frac {SS_{res}}  {{SS}_{tot}})(\\frac {n - 1} {n - p - 1})}$$\n",
    "\n",
    "...where $p$ is the number of regressors and $n$ is the sample size. As a result, a higher sample size will negate the effects of a higher regressor count. Still, a higher regressor count will reduce the size of the denominator, increase the ratio, and produce a smaller $\\text{Adjusted R}^2$ value if the error doesn't also reduce significantly.\n",
    "\n",
    "__Code:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
